
# ðŸ“Š GCP Dataflow Pipeline: CSV to BigQuery

This project demonstrates how to create a **data ingestion pipeline** using **Google Cloud Dataflow**, which reads a CSV file from **Google Cloud Storage (GCS)**, transforms the data using a **JavaScript UDF**, and writes the results to a **BigQuery** table.

The dataset contains fictional employee information such as names, job titles, departments, and salaries.

---

## ðŸ“ Project Structure

```
ðŸ“¦ dataflow-pipeline/
 â”£ ðŸ“„ bq.json         â†’ Defines BigQuery schema for the target table
 â”£ ðŸ“„ udf.js          â†’ JavaScript UDF for row transformation
 â”£ ðŸ“„ employee_data.csv (stored in GCS bucket)
```

---

## ðŸš€ Technologies Used

- **Google Cloud Dataflow**
- **Google Cloud Storage (GCS)**
- **BigQuery**
- **JavaScript (UDF)**
- **JSON (Schema definition)**

---

## ðŸ”„ Pipeline Workflow

1. **CSV File** is uploaded to a GCS bucket.
2. **Dataflow job** is triggered:
   - Reads the CSV file.
   - Applies transformations using a **JavaScript UDF**.
   - Converts rows to **JSON objects** matching the BigQuery schema.
3. Transformed data is loaded into a **BigQuery table**.

---

## ðŸ§  Key Learnings

- Setting up a **Dataflow pipeline** for ETL tasks.
- Using **JavaScript UDFs** to clean and transform data.
- Defining **BigQuery schemas** with JSON.
- Moving data from **Cloud Storage to BigQuery** efficiently.

---

## ðŸ“¹ Reference

This project follows the tutorial by **Vishal Bulbule**'s channel '**TechTrapture**' on YouTube:

> [Google Cloud Dataflow Explained | Create Your First Data Pipeline (GCS to BigQuery)](https://www.youtube.com/watch?v=3dMSI-UDFfA)

---

